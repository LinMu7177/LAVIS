/home/tsingqguo/anaconda3/envs/lavis_v2/lib/python3.8/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
2023-09-18 07:41:19,630 [INFO] 
=====  Running Parameters    =====
2023-09-18 07:41:19,631 [INFO] {
    "amp": false,
    "batch_size_eval": 4,
    "batch_size_train": 32,
    "device": "cuda",
    "dist_url": "env://",
    "distributed": false,
    "evaluate": false,
    "inference_method": "generate",
    "init_lr": 2e-05,
    "lr_sched": "linear_warmup_cosine_lr",
    "max_epoch": 10,
    "max_len": 10,
    "min_len": 1,
    "min_lr": 0,
    "num_ans_candidates": 128,
    "num_beams": 5,
    "num_workers": 4,
    "output_dir": "output/BLIP/VQA/mnt/local/wwx/Output/LAVIS/VQA_FT",
    "resume_ckpt_path": null,
    "seed": 42,
    "task": "vqa",
    "test_splits": [
        "test"
    ],
    "train_splits": [
        "train"
    ],
    "weight_decay": 0.05,
    "world_size": 2
}
2023-09-18 07:41:19,631 [INFO] 
======  Dataset Attributes  ======
2023-09-18 07:41:19,631 [INFO] 
======== coco_vqa =======
2023-09-18 07:41:19,632 [INFO] {
    "build_info": {
        "annotations": {
            "test": {
                "storage": [
                    "coco/annotations/vqa_test.json",
                    "coco/annotations/answer_list.json"
                ],
                "url": [
                    "https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/vqav2/vqa_test.json",
                    "https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/vqav2/answer_list.json"
                ]
            },
            "train": {
                "storage": [
                    "coco/annotations/vqa_train.json",
                    "coco/annotations/vqa_val.json"
                ],
                "url": [
                    "https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/vqav2/vqa_train.json",
                    "https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/vqav2/vqa_val.json"
                ]
            },
            "val": {
                "storage": [
                    "coco/annotations/vqa_val_eval.json",
                    "coco/annotations/answer_list.json",
                    "coco/annotations/v2_OpenEnded_mscoco_val2014_questions.json",
                    "coco/annotations/v2_mscoco_val2014_annotations.json"
                ],
                "url": [
                    "https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/vqav2/vqa_val_eval.json",
                    "https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/vqav2/answer_list.json",
                    "https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/vqav2/v2_OpenEnded_mscoco_val2014_questions.json",
                    "https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/vqav2/v2_mscoco_val2014_annotations.json"
                ]
            }
        },
        "images": {
            "storage": "coco/images/"
        }
    },
    "data_type": "images",
    "text_processor": {
        "eval": {
            "name": "blip_question"
        },
        "train": {
            "name": "blip_question"
        }
    },
    "vis_processor": {
        "eval": {
            "image_size": 224,
            "name": "blip_image_eval"
        },
        "train": {
            "image_size": 224,
            "name": "blip_image_train"
        }
    }
}
2023-09-18 07:41:19,632 [INFO] 
======  Model Attributes  ======
2023-09-18 07:41:19,632 [INFO] {
    "arch": "blip2_vicuna_instruct",
    "drop_path_rate": 0,
    "finetuned": "",
    "freeze_vit": true,
    "image_size": 224,
    "llm_model": "/mnt/local/wwx/Models/Vicuna/vicuna-7b/",
    "load_finetuned": false,
    "load_pretrained": true,
    "model_type": "vicuna7b",
    "num_query_token": 32,
    "pretrained": "https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/InstructBLIP/instruct_blip_vicuna7b_trimmed.pth",
    "prompt": "",
    "use_grad_checkpoint": false,
    "vit_precision": "fp16"
}
2023-09-18 07:41:19,637 [INFO] Building datasets...
2023-09-18 07:41:45,819 [INFO] freeze vision encoder
Not using distributed mode
Using downloaded and verified file: /mnt/local/wwx/LLM_Data/Lavis_V2/coco/annotations/vqa_train.json
Using downloaded and verified file: /mnt/local/wwx/LLM_Data/Lavis_V2/coco/annotations/vqa_val.json
Using downloaded and verified file: /mnt/local/wwx/LLM_Data/Lavis_V2/coco/annotations/vqa_val_eval.json
Using downloaded and verified file: /mnt/local/wwx/LLM_Data/Lavis_V2/coco/annotations/answer_list.json
Using downloaded and verified file: /mnt/local/wwx/LLM_Data/Lavis_V2/coco/annotations/v2_OpenEnded_mscoco_val2014_questions.json
Using downloaded and verified file: /mnt/local/wwx/LLM_Data/Lavis_V2/coco/annotations/v2_mscoco_val2014_annotations.json
Using downloaded and verified file: /mnt/local/wwx/LLM_Data/Lavis_V2/coco/annotations/vqa_test.json
Using downloaded and verified file: /mnt/local/wwx/LLM_Data/Lavis_V2/coco/annotations/answer_list.json
Loading LLAMA
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.24s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.19s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.49s/it]
2023-09-18 07:44:23,981 [INFO] load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/InstructBLIP/instruct_blip_vicuna7b_trimmed.pth
2023-09-18 07:44:24,088 [INFO] Start training
2023-09-18 07:44:28,802 [INFO] dataset_ratios not specified, datasets will be concatenated (map-style datasets) or chained (webdataset.DataPipeline).
2023-09-18 07:44:28,802 [INFO] Loaded 658104 records for train split from the dataset.
2023-09-18 07:44:28,802 [INFO] Loaded 214354 records for val split from the dataset.
2023-09-18 07:44:28,802 [INFO] Loaded 447793 records for test split from the dataset.
2023-09-18 07:44:28,810 [INFO] number of trainable parameters: 188837376
2023-09-18 07:44:28,811 [INFO] Start training epoch 0, 20565 iters per inner epoch.
Loading LLAMA Done
/home/tsingqguo/wwx/paper_worksapce/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/tsingqguo/wwx/paper_worksapce/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/tsingqguo/wwx/paper_worksapce/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/tsingqguo/wwx/paper_worksapce/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/tsingqguo/anaconda3/envs/lavis_v2/lib/python3.8/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
Not using distributed mode
2023-09-18 08:02:37,826 [INFO] 
=====  Running Parameters    =====
2023-09-18 08:02:37,826 [INFO] {
    "amp": false,
    "batch_size_eval": 4,
    "batch_size_train": 32,
    "device": "cuda",
    "dist_url": "env://",
    "distributed": false,
    "evaluate": false,
    "inference_method": "generate",
    "init_lr": 2e-05,
    "lr_sched": "linear_warmup_cosine_lr",
    "max_epoch": 10,
    "max_len": 10,
    "min_len": 1,
    "min_lr": 0,
    "num_ans_candidates": 128,
    "num_beams": 5,
    "num_workers": 4,
    "output_dir": "output/BLIP/VQA/mnt/local/wwx/Output/LAVIS/VQA_FT",
    "resume_ckpt_path": null,
    "seed": 42,
    "task": "vqa",
    "test_splits": [
        "test"
    ],
    "train_splits": [
        "train"
    ],
    "weight_decay": 0.05,
    "world_size": 2
}
2023-09-18 08:02:37,826 [INFO] 
======  Dataset Attributes  ======
2023-09-18 08:02:37,826 [INFO] 
======== coco_vqa =======
2023-09-18 08:02:37,827 [INFO] {
    "build_info": {
        "annotations": {
            "test": {
                "storage": [
                    "coco/annotations/vqa_test.json",
                    "coco/annotations/answer_list.json"
                ],
                "url": [
                    "https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/vqav2/vqa_test.json",
                    "https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/vqav2/answer_list.json"
                ]
            },
            "train": {
                "storage": [
                    "coco/annotations/vqa_train.json",
                    "coco/annotations/vqa_val.json"
                ],
                "url": [
                    "https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/vqav2/vqa_train.json",
                    "https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/vqav2/vqa_val.json"
                ]
            },
            "val": {
                "storage": [
                    "coco/annotations/vqa_val_eval.json",
                    "coco/annotations/answer_list.json",
                    "coco/annotations/v2_OpenEnded_mscoco_val2014_questions.json",
                    "coco/annotations/v2_mscoco_val2014_annotations.json"
                ],
                "url": [
                    "https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/vqav2/vqa_val_eval.json",
                    "https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/vqav2/answer_list.json",
                    "https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/vqav2/v2_OpenEnded_mscoco_val2014_questions.json",
                    "https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/vqav2/v2_mscoco_val2014_annotations.json"
                ]
            }
        },
        "images": {
            "storage": "coco/images/"
        }
    },
    "data_type": "images",
    "text_processor": {
        "eval": {
            "name": "blip_question"
        },
        "train": {
            "name": "blip_question"
        }
    },
    "vis_processor": {
        "eval": {
            "image_size": 224,
            "name": "blip_image_eval"
        },
        "train": {
            "image_size": 224,
            "name": "blip_image_train"
        }
    }
}
2023-09-18 08:02:37,827 [INFO] 
======  Model Attributes  ======
2023-09-18 08:02:37,828 [INFO] {
    "arch": "blip2_vicuna_instruct",
    "drop_path_rate": 0,
    "finetuned": "",
    "freeze_vit": true,
    "image_size": 224,
    "llm_model": "/mnt/local/wwx/Models/Vicuna/vicuna-7b/",
    "load_finetuned": false,
    "load_pretrained": true,
    "model_type": "vicuna7b",
    "num_query_token": 32,
    "pretrained": "https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/InstructBLIP/instruct_blip_vicuna7b_trimmed.pth",
    "prompt": "",
    "use_grad_checkpoint": false,
    "vit_precision": "fp16"
}
Using downloaded and verified file: /mnt/local/wwx/LLM_Data/Lavis_V2/coco/annotations/vqa_train.json
Using downloaded and verified file: /mnt/local/wwx/LLM_Data/Lavis_V2/coco/annotations/vqa_val.json
Using downloaded and verified file: /mnt/local/wwx/LLM_Data/Lavis_V2/coco/annotations/vqa_val_eval.json
Using downloaded and verified file: /mnt/local/wwx/LLM_Data/Lavis_V2/coco/annotations/answer_list.json
Using downloaded and verified file: /mnt/local/wwx/LLM_Data/Lavis_V2/coco/annotations/v2_OpenEnded_mscoco_val2014_questions.json
Using downloaded and verified file: /mnt/local/wwx/LLM_Data/Lavis_V2/coco/annotations/v2_mscoco_val2014_annotations.json
Using downloaded and verified file: /mnt/local/wwx/LLM_Data/Lavis_V2/coco/annotations/vqa_test.json
Using downloaded and verified file: /mnt/local/wwx/LLM_Data/Lavis_V2/coco/annotations/answer_list.json
2023-09-18 08:02:37,835 [INFO] Building datasets...
2023-09-18 08:03:03,074 [INFO] freeze vision encoder
Loading LLAMA
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:24<00:24, 24.48s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:29<00:00, 13.26s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:29<00:00, 14.94s/it]
Loading LLAMA Done
2023-09-18 08:06:06,427 [INFO] load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/InstructBLIP/instruct_blip_vicuna7b_trimmed.pth
2023-09-18 08:06:06,444 [INFO] Start training
2023-09-18 08:06:12,629 [INFO] dataset_ratios not specified, datasets will be concatenated (map-style datasets) or chained (webdataset.DataPipeline).
2023-09-18 08:06:12,629 [INFO] Loaded 658104 records for train split from the dataset.
2023-09-18 08:06:12,629 [INFO] Loaded 214354 records for val split from the dataset.
2023-09-18 08:06:12,629 [INFO] Loaded 447793 records for test split from the dataset.
2023-09-18 08:06:12,637 [INFO] number of trainable parameters: 188837376
2023-09-18 08:06:12,638 [INFO] Start training epoch 0, 20565 iters per inner epoch.
/home/tsingqguo/wwx/paper_worksapce/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/tsingqguo/wwx/paper_worksapce/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/tsingqguo/wwx/paper_worksapce/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/tsingqguo/wwx/paper_worksapce/LAVIS/lavis/processors/randaugment.py:40: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Train: data epoch: [0]  [    0/20565]  eta: 19:28:56  lr: 0.000020  loss: 5.5363  time: 3.4105  data: 0.0000  max mem: 28985
Train: data epoch: [0]  [   50/20565]  eta: 4:16:33  lr: 0.000020  loss: 3.2313  time: 0.7036  data: 0.0000  max mem: 31260
Train: data epoch: [0]  [  100/20565]  eta: 4:06:32  lr: 0.000020  loss: 2.8773  time: 0.6809  data: 0.0000  max mem: 31263
Train: data epoch: [0]  [  150/20565]  eta: 4:03:27  lr: 0.000020  loss: 3.3920  time: 0.6976  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [  200/20565]  eta: 4:02:34  lr: 0.000020  loss: 2.6244  time: 0.7129  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [  250/20565]  eta: 4:01:45  lr: 0.000020  loss: 2.7397  time: 0.7163  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [  300/20565]  eta: 4:00:21  lr: 0.000020  loss: 2.7676  time: 0.6888  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [  350/20565]  eta: 3:55:57  lr: 0.000020  loss: 2.9479  time: 0.6344  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [  400/20565]  eta: 3:52:38  lr: 0.000020  loss: 2.8185  time: 0.6367  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [  450/20565]  eta: 3:49:56  lr: 0.000020  loss: 2.8955  time: 0.6438  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [  500/20565]  eta: 3:47:51  lr: 0.000020  loss: 3.0406  time: 0.6402  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [  550/20565]  eta: 3:46:01  lr: 0.000020  loss: 2.7925  time: 0.6375  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [  600/20565]  eta: 3:44:22  lr: 0.000020  loss: 2.9795  time: 0.6445  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [  650/20565]  eta: 3:42:53  lr: 0.000020  loss: 2.7108  time: 0.6406  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [  700/20565]  eta: 3:41:30  lr: 0.000020  loss: 1.8861  time: 0.6366  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [  750/20565]  eta: 3:40:16  lr: 0.000020  loss: 3.3210  time: 0.6387  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [  800/20565]  eta: 3:39:04  lr: 0.000020  loss: 2.9800  time: 0.6286  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [  850/20565]  eta: 3:37:58  lr: 0.000020  loss: 3.0630  time: 0.6362  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [  900/20565]  eta: 3:36:54  lr: 0.000020  loss: 2.8936  time: 0.6399  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [  950/20565]  eta: 3:35:58  lr: 0.000020  loss: 2.9781  time: 0.6400  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 1000/20565]  eta: 3:35:02  lr: 0.000020  loss: 3.2441  time: 0.6397  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 1050/20565]  eta: 3:34:05  lr: 0.000020  loss: 2.9201  time: 0.6312  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 1100/20565]  eta: 3:33:12  lr: 0.000020  loss: 2.4735  time: 0.6394  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 1150/20565]  eta: 3:32:23  lr: 0.000020  loss: 2.7966  time: 0.6306  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 1200/20565]  eta: 3:31:30  lr: 0.000020  loss: 2.9680  time: 0.6349  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 1250/20565]  eta: 3:30:44  lr: 0.000020  loss: 2.8303  time: 0.6353  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 1300/20565]  eta: 3:29:59  lr: 0.000020  loss: 2.2723  time: 0.6428  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 1350/20565]  eta: 3:29:17  lr: 0.000020  loss: 2.7448  time: 0.6407  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 1400/20565]  eta: 3:28:38  lr: 0.000020  loss: 3.0891  time: 0.6475  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 1450/20565]  eta: 3:27:54  lr: 0.000020  loss: 3.2695  time: 0.6409  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 1500/20565]  eta: 3:27:12  lr: 0.000020  loss: 2.6050  time: 0.6365  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 1550/20565]  eta: 3:26:27  lr: 0.000020  loss: 3.0559  time: 0.6322  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 1600/20565]  eta: 3:25:42  lr: 0.000020  loss: 2.3240  time: 0.6328  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 1650/20565]  eta: 3:25:03  lr: 0.000020  loss: 2.7829  time: 0.6386  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 1700/20565]  eta: 3:24:23  lr: 0.000020  loss: 3.2177  time: 0.6423  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 1750/20565]  eta: 3:23:45  lr: 0.000020  loss: 2.2826  time: 0.6365  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 1800/20565]  eta: 3:23:04  lr: 0.000020  loss: 2.1027  time: 0.6289  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 1850/20565]  eta: 3:22:26  lr: 0.000020  loss: 2.9039  time: 0.6447  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 1900/20565]  eta: 3:21:46  lr: 0.000020  loss: 3.1623  time: 0.6422  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 1950/20565]  eta: 3:21:10  lr: 0.000020  loss: 2.4985  time: 0.6448  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 2000/20565]  eta: 3:20:34  lr: 0.000020  loss: 2.2606  time: 0.6388  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 2050/20565]  eta: 3:19:55  lr: 0.000020  loss: 3.0970  time: 0.6342  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 2100/20565]  eta: 3:19:17  lr: 0.000020  loss: 2.7285  time: 0.6382  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 2150/20565]  eta: 3:18:41  lr: 0.000020  loss: 2.9560  time: 0.6353  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 2200/20565]  eta: 3:18:04  lr: 0.000020  loss: 2.2683  time: 0.6344  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 2250/20565]  eta: 3:17:31  lr: 0.000020  loss: 3.4383  time: 0.6541  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 2300/20565]  eta: 3:16:56  lr: 0.000020  loss: 3.5845  time: 0.6490  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 2350/20565]  eta: 3:16:20  lr: 0.000020  loss: 2.7451  time: 0.6388  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 2400/20565]  eta: 3:15:54  lr: 0.000020  loss: 3.0750  time: 0.6332  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 2450/20565]  eta: 3:15:19  lr: 0.000020  loss: 2.3127  time: 0.6354  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 2500/20565]  eta: 3:14:43  lr: 0.000020  loss: 3.1179  time: 0.6346  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 2550/20565]  eta: 3:14:07  lr: 0.000020  loss: 3.0085  time: 0.6363  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 2600/20565]  eta: 3:13:33  lr: 0.000020  loss: 2.5577  time: 0.6415  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 2650/20565]  eta: 3:13:00  lr: 0.000020  loss: 2.4366  time: 0.6381  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 2700/20565]  eta: 3:12:25  lr: 0.000020  loss: 3.5344  time: 0.6367  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 2750/20565]  eta: 3:11:48  lr: 0.000020  loss: 3.0287  time: 0.6381  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 2800/20565]  eta: 3:11:12  lr: 0.000020  loss: 3.0507  time: 0.6373  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 2850/20565]  eta: 3:10:37  lr: 0.000020  loss: 2.8871  time: 0.6364  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 2900/20565]  eta: 3:10:03  lr: 0.000020  loss: 2.7865  time: 0.6398  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 2950/20565]  eta: 3:09:28  lr: 0.000020  loss: 2.3786  time: 0.6308  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 3000/20565]  eta: 3:08:52  lr: 0.000020  loss: 2.4299  time: 0.6430  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 3050/20565]  eta: 3:08:16  lr: 0.000020  loss: 2.9487  time: 0.6274  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 3100/20565]  eta: 3:07:47  lr: 0.000020  loss: 2.7327  time: 0.6323  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 3150/20565]  eta: 3:07:13  lr: 0.000020  loss: 2.7026  time: 0.6419  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 3200/20565]  eta: 3:06:45  lr: 0.000020  loss: 3.0227  time: 0.6365  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 3250/20565]  eta: 3:06:11  lr: 0.000020  loss: 2.8878  time: 0.6365  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 3300/20565]  eta: 3:05:36  lr: 0.000020  loss: 3.0048  time: 0.6357  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 3350/20565]  eta: 3:05:02  lr: 0.000020  loss: 3.3154  time: 0.6457  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 3400/20565]  eta: 3:04:28  lr: 0.000020  loss: 2.3053  time: 0.6395  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 3450/20565]  eta: 3:03:57  lr: 0.000020  loss: 2.9626  time: 0.6464  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 3500/20565]  eta: 3:03:24  lr: 0.000020  loss: 3.7118  time: 0.6447  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 3550/20565]  eta: 3:02:49  lr: 0.000020  loss: 2.5454  time: 0.6364  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 3600/20565]  eta: 3:02:14  lr: 0.000020  loss: 2.8213  time: 0.6361  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 3650/20565]  eta: 3:01:39  lr: 0.000020  loss: 2.7672  time: 0.6344  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 3700/20565]  eta: 3:01:04  lr: 0.000020  loss: 2.5834  time: 0.6366  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 3750/20565]  eta: 3:00:30  lr: 0.000020  loss: 3.1668  time: 0.6368  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 3800/20565]  eta: 3:00:03  lr: 0.000020  loss: 2.7269  time: 0.7203  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 3850/20565]  eta: 2:59:26  lr: 0.000020  loss: 2.5111  time: 0.6248  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 3900/20565]  eta: 2:58:51  lr: 0.000020  loss: 3.1174  time: 0.6303  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 3950/20565]  eta: 2:58:16  lr: 0.000020  loss: 3.1798  time: 0.6305  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 4000/20565]  eta: 2:57:41  lr: 0.000020  loss: 2.8420  time: 0.6330  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 4050/20565]  eta: 2:57:05  lr: 0.000020  loss: 3.4277  time: 0.6191  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 4100/20565]  eta: 2:56:31  lr: 0.000020  loss: 2.4202  time: 0.6315  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 4150/20565]  eta: 2:56:00  lr: 0.000020  loss: 2.8066  time: 0.6611  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 4200/20565]  eta: 2:55:26  lr: 0.000020  loss: 2.7143  time: 0.6370  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 4250/20565]  eta: 2:54:55  lr: 0.000020  loss: 3.1505  time: 0.6650  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 4300/20565]  eta: 2:54:23  lr: 0.000020  loss: 2.8784  time: 0.6600  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 4350/20565]  eta: 2:53:49  lr: 0.000020  loss: 2.4998  time: 0.6296  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 4400/20565]  eta: 2:53:17  lr: 0.000020  loss: 2.8529  time: 0.6429  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 4450/20565]  eta: 2:52:42  lr: 0.000020  loss: 2.9056  time: 0.6355  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 4500/20565]  eta: 2:52:10  lr: 0.000020  loss: 2.4999  time: 0.6381  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 4550/20565]  eta: 2:51:37  lr: 0.000020  loss: 2.4249  time: 0.6465  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 4600/20565]  eta: 2:51:04  lr: 0.000020  loss: 2.7542  time: 0.6334  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 4650/20565]  eta: 2:50:32  lr: 0.000020  loss: 2.6415  time: 0.6516  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 4700/20565]  eta: 2:49:58  lr: 0.000020  loss: 2.9400  time: 0.6334  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 4750/20565]  eta: 2:49:26  lr: 0.000020  loss: 2.9986  time: 0.6401  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 4800/20565]  eta: 2:48:53  lr: 0.000020  loss: 2.6022  time: 0.6389  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 4850/20565]  eta: 2:48:20  lr: 0.000020  loss: 2.2081  time: 0.6350  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 4900/20565]  eta: 2:47:46  lr: 0.000020  loss: 2.7975  time: 0.6312  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 4950/20565]  eta: 2:47:13  lr: 0.000020  loss: 2.5784  time: 0.6359  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 5000/20565]  eta: 2:46:40  lr: 0.000020  loss: 2.9715  time: 0.6345  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 5050/20565]  eta: 2:46:06  lr: 0.000020  loss: 2.5478  time: 0.6302  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 5100/20565]  eta: 2:45:33  lr: 0.000020  loss: 3.2030  time: 0.6331  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 5150/20565]  eta: 2:45:00  lr: 0.000020  loss: 2.7334  time: 0.6372  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 5200/20565]  eta: 2:44:27  lr: 0.000020  loss: 2.7835  time: 0.6319  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 5250/20565]  eta: 2:43:53  lr: 0.000020  loss: 2.7891  time: 0.6300  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 5300/20565]  eta: 2:43:20  lr: 0.000020  loss: 3.2312  time: 0.6384  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 5350/20565]  eta: 2:42:46  lr: 0.000020  loss: 2.4215  time: 0.6358  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 5400/20565]  eta: 2:42:13  lr: 0.000020  loss: 3.3494  time: 0.6352  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 5450/20565]  eta: 2:41:40  lr: 0.000020  loss: 3.4159  time: 0.6328  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 5500/20565]  eta: 2:41:07  lr: 0.000020  loss: 3.1987  time: 0.6298  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 5550/20565]  eta: 2:40:34  lr: 0.000020  loss: 2.4509  time: 0.6368  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 5600/20565]  eta: 2:40:01  lr: 0.000020  loss: 2.0684  time: 0.6336  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 5650/20565]  eta: 2:39:28  lr: 0.000020  loss: 2.5259  time: 0.6339  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 5700/20565]  eta: 2:38:55  lr: 0.000020  loss: 3.3670  time: 0.6302  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 5750/20565]  eta: 2:38:22  lr: 0.000020  loss: 3.0001  time: 0.6380  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 5800/20565]  eta: 2:37:49  lr: 0.000020  loss: 2.1797  time: 0.6363  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 5850/20565]  eta: 2:37:17  lr: 0.000020  loss: 2.6235  time: 0.6430  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 5900/20565]  eta: 2:36:44  lr: 0.000020  loss: 2.7000  time: 0.6372  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 5950/20565]  eta: 2:36:11  lr: 0.000020  loss: 2.8683  time: 0.6332  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 6000/20565]  eta: 2:35:39  lr: 0.000020  loss: 2.4091  time: 0.6386  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 6050/20565]  eta: 2:35:06  lr: 0.000020  loss: 2.7807  time: 0.6290  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 6100/20565]  eta: 2:34:33  lr: 0.000020  loss: 1.9658  time: 0.6405  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 6150/20565]  eta: 2:34:01  lr: 0.000020  loss: 3.1755  time: 0.6383  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 6200/20565]  eta: 2:33:28  lr: 0.000020  loss: 2.3219  time: 0.6372  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 6250/20565]  eta: 2:32:56  lr: 0.000020  loss: 3.2183  time: 0.6459  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 6300/20565]  eta: 2:32:24  lr: 0.000020  loss: 3.1418  time: 0.6376  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 6350/20565]  eta: 2:31:51  lr: 0.000020  loss: 3.4457  time: 0.6332  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 6400/20565]  eta: 2:31:18  lr: 0.000020  loss: 2.7522  time: 0.6394  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 6450/20565]  eta: 2:30:45  lr: 0.000020  loss: 3.3161  time: 0.6362  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 6500/20565]  eta: 2:30:13  lr: 0.000020  loss: 2.7405  time: 0.6393  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 6550/20565]  eta: 2:29:40  lr: 0.000020  loss: 2.2622  time: 0.6305  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 6600/20565]  eta: 2:29:08  lr: 0.000020  loss: 2.8781  time: 0.6287  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 6650/20565]  eta: 2:28:35  lr: 0.000020  loss: 2.8004  time: 0.6428  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 6700/20565]  eta: 2:28:02  lr: 0.000020  loss: 3.7322  time: 0.6325  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 6750/20565]  eta: 2:27:30  lr: 0.000020  loss: 3.8731  time: 0.6386  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 6800/20565]  eta: 2:26:58  lr: 0.000020  loss: 3.6205  time: 0.6373  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 6850/20565]  eta: 2:26:25  lr: 0.000020  loss: 3.0091  time: 0.6395  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 6900/20565]  eta: 2:25:53  lr: 0.000020  loss: 3.1539  time: 0.6338  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 6950/20565]  eta: 2:25:20  lr: 0.000020  loss: 2.6769  time: 0.6325  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 7000/20565]  eta: 2:24:47  lr: 0.000020  loss: 2.8469  time: 0.6354  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 7050/20565]  eta: 2:24:15  lr: 0.000020  loss: 2.9816  time: 0.6320  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 7100/20565]  eta: 2:23:43  lr: 0.000020  loss: 3.0077  time: 0.6489  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 7150/20565]  eta: 2:23:11  lr: 0.000020  loss: 2.7757  time: 0.6459  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 7200/20565]  eta: 2:22:38  lr: 0.000020  loss: 2.8932  time: 0.6377  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 7250/20565]  eta: 2:22:06  lr: 0.000020  loss: 2.9534  time: 0.6320  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 7300/20565]  eta: 2:21:33  lr: 0.000020  loss: 3.2649  time: 0.6314  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 7350/20565]  eta: 2:21:01  lr: 0.000020  loss: 3.0470  time: 0.6295  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 7400/20565]  eta: 2:20:29  lr: 0.000020  loss: 2.6240  time: 0.6478  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 7450/20565]  eta: 2:19:56  lr: 0.000020  loss: 2.4135  time: 0.6305  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 7500/20565]  eta: 2:19:24  lr: 0.000020  loss: 2.5812  time: 0.6341  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 7550/20565]  eta: 2:18:52  lr: 0.000020  loss: 3.0593  time: 0.6309  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 7600/20565]  eta: 2:18:23  lr: 0.000020  loss: 2.8467  time: 0.6385  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 7650/20565]  eta: 2:17:51  lr: 0.000020  loss: 4.1188  time: 0.6311  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 7700/20565]  eta: 2:17:18  lr: 0.000020  loss: 2.9108  time: 0.6385  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 7750/20565]  eta: 2:16:46  lr: 0.000020  loss: 3.2673  time: 0.6378  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 7800/20565]  eta: 2:16:14  lr: 0.000020  loss: 3.1187  time: 0.6384  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 7850/20565]  eta: 2:15:41  lr: 0.000020  loss: 3.4534  time: 0.6368  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 7900/20565]  eta: 2:15:09  lr: 0.000020  loss: 3.0798  time: 0.6321  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 7950/20565]  eta: 2:14:37  lr: 0.000020  loss: 2.8550  time: 0.6427  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 8000/20565]  eta: 2:14:04  lr: 0.000020  loss: 2.4343  time: 0.6385  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 8050/20565]  eta: 2:13:32  lr: 0.000020  loss: 2.7462  time: 0.6395  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 8100/20565]  eta: 2:13:00  lr: 0.000020  loss: 3.3175  time: 0.6279  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 8150/20565]  eta: 2:12:28  lr: 0.000020  loss: 2.9157  time: 0.6377  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 8200/20565]  eta: 2:11:55  lr: 0.000020  loss: 2.3705  time: 0.6377  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 8250/20565]  eta: 2:11:23  lr: 0.000020  loss: 3.3267  time: 0.6428  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 8300/20565]  eta: 2:10:51  lr: 0.000020  loss: 2.2268  time: 0.6355  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 8350/20565]  eta: 2:10:18  lr: 0.000020  loss: 2.2485  time: 0.6331  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 8400/20565]  eta: 2:09:46  lr: 0.000020  loss: 2.4928  time: 0.6216  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 8450/20565]  eta: 2:09:13  lr: 0.000020  loss: 2.4043  time: 0.6360  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 8500/20565]  eta: 2:08:41  lr: 0.000020  loss: 2.5999  time: 0.6399  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 8550/20565]  eta: 2:08:09  lr: 0.000020  loss: 3.8756  time: 0.6339  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 8600/20565]  eta: 2:07:36  lr: 0.000020  loss: 2.9283  time: 0.6267  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 8650/20565]  eta: 2:07:04  lr: 0.000020  loss: 2.1229  time: 0.6382  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 8700/20565]  eta: 2:06:31  lr: 0.000020  loss: 2.4938  time: 0.6299  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 8750/20565]  eta: 2:05:59  lr: 0.000020  loss: 2.7798  time: 0.6313  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 8800/20565]  eta: 2:05:27  lr: 0.000020  loss: 3.0284  time: 0.6391  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 8850/20565]  eta: 2:04:55  lr: 0.000020  loss: 2.5386  time: 0.6390  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 8900/20565]  eta: 2:04:23  lr: 0.000020  loss: 2.6894  time: 0.6517  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 8950/20565]  eta: 2:03:51  lr: 0.000020  loss: 2.5262  time: 0.6329  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 9000/20565]  eta: 2:03:18  lr: 0.000020  loss: 2.1697  time: 0.6338  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 9050/20565]  eta: 2:02:46  lr: 0.000020  loss: 3.0351  time: 0.6362  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 9100/20565]  eta: 2:02:14  lr: 0.000020  loss: 2.7507  time: 0.6336  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 9150/20565]  eta: 2:01:42  lr: 0.000020  loss: 2.9631  time: 0.6362  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 9200/20565]  eta: 2:01:10  lr: 0.000020  loss: 2.7973  time: 0.6358  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 9250/20565]  eta: 2:00:37  lr: 0.000020  loss: 3.1285  time: 0.6386  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 9300/20565]  eta: 2:00:05  lr: 0.000020  loss: 2.5244  time: 0.6398  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 9350/20565]  eta: 1:59:32  lr: 0.000020  loss: 2.8537  time: 0.6254  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 9400/20565]  eta: 1:59:00  lr: 0.000020  loss: 2.8780  time: 0.6329  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 9450/20565]  eta: 1:58:28  lr: 0.000020  loss: 3.1502  time: 0.6345  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 9500/20565]  eta: 1:57:55  lr: 0.000020  loss: 2.9314  time: 0.6321  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 9550/20565]  eta: 1:57:23  lr: 0.000020  loss: 2.9028  time: 0.6311  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 9600/20565]  eta: 1:56:51  lr: 0.000020  loss: 2.3331  time: 0.6323  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 9650/20565]  eta: 1:56:19  lr: 0.000020  loss: 2.8350  time: 0.6340  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 9700/20565]  eta: 1:55:46  lr: 0.000020  loss: 2.8095  time: 0.6336  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 9750/20565]  eta: 1:55:14  lr: 0.000020  loss: 2.8687  time: 0.6306  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 9800/20565]  eta: 1:54:42  lr: 0.000020  loss: 2.4152  time: 0.6334  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 9850/20565]  eta: 1:54:10  lr: 0.000020  loss: 3.4834  time: 0.6361  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 9900/20565]  eta: 1:53:38  lr: 0.000020  loss: 2.5179  time: 0.6387  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [ 9950/20565]  eta: 1:53:06  lr: 0.000020  loss: 2.8545  time: 0.6353  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [10000/20565]  eta: 1:52:34  lr: 0.000020  loss: 2.6219  time: 0.6442  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [10050/20565]  eta: 1:52:02  lr: 0.000020  loss: 3.1540  time: 0.6358  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [10100/20565]  eta: 1:51:30  lr: 0.000020  loss: 3.0293  time: 0.6440  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [10150/20565]  eta: 1:50:58  lr: 0.000020  loss: 2.8248  time: 0.6354  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [10200/20565]  eta: 1:50:25  lr: 0.000020  loss: 2.4030  time: 0.6341  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [10250/20565]  eta: 1:49:53  lr: 0.000020  loss: 3.3286  time: 0.6353  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [10300/20565]  eta: 1:49:21  lr: 0.000020  loss: 2.7885  time: 0.6393  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [10350/20565]  eta: 1:48:49  lr: 0.000020  loss: 2.9029  time: 0.6330  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [10400/20565]  eta: 1:48:17  lr: 0.000020  loss: 3.3421  time: 0.6489  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [10450/20565]  eta: 1:47:45  lr: 0.000020  loss: 2.9476  time: 0.6354  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [10500/20565]  eta: 1:47:13  lr: 0.000020  loss: 2.5657  time: 0.6341  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [10550/20565]  eta: 1:46:41  lr: 0.000020  loss: 2.9439  time: 0.6382  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [10600/20565]  eta: 1:46:09  lr: 0.000020  loss: 2.5093  time: 0.6335  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [10650/20565]  eta: 1:45:37  lr: 0.000020  loss: 2.6364  time: 0.6372  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [10700/20565]  eta: 1:45:05  lr: 0.000020  loss: 2.7374  time: 0.6335  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [10750/20565]  eta: 1:44:32  lr: 0.000020  loss: 2.7833  time: 0.6276  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [10800/20565]  eta: 1:44:00  lr: 0.000020  loss: 2.6893  time: 0.6426  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [10850/20565]  eta: 1:43:28  lr: 0.000020  loss: 3.1580  time: 0.6325  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [10900/20565]  eta: 1:42:56  lr: 0.000020  loss: 2.6243  time: 0.6345  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [10950/20565]  eta: 1:42:24  lr: 0.000020  loss: 2.6139  time: 0.6441  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [11000/20565]  eta: 1:41:52  lr: 0.000020  loss: 3.3554  time: 0.6360  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [11050/20565]  eta: 1:41:20  lr: 0.000020  loss: 2.4091  time: 0.6340  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [11100/20565]  eta: 1:40:48  lr: 0.000020  loss: 2.6620  time: 0.6345  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [11150/20565]  eta: 1:40:15  lr: 0.000020  loss: 2.7296  time: 0.6312  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [11200/20565]  eta: 1:39:43  lr: 0.000020  loss: 2.6189  time: 0.6416  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [11250/20565]  eta: 1:39:11  lr: 0.000020  loss: 2.7487  time: 0.6373  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [11300/20565]  eta: 1:38:39  lr: 0.000020  loss: 2.3895  time: 0.6428  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [11350/20565]  eta: 1:38:07  lr: 0.000020  loss: 3.0827  time: 0.6307  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [11400/20565]  eta: 1:37:35  lr: 0.000020  loss: 2.7929  time: 0.6334  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [11450/20565]  eta: 1:37:03  lr: 0.000020  loss: 3.2598  time: 0.6398  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [11500/20565]  eta: 1:36:32  lr: 0.000020  loss: 2.9598  time: 0.6418  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [11550/20565]  eta: 1:36:00  lr: 0.000020  loss: 2.7768  time: 0.6334  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [11600/20565]  eta: 1:35:28  lr: 0.000020  loss: 3.1541  time: 0.6315  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [11650/20565]  eta: 1:34:56  lr: 0.000020  loss: 3.3200  time: 0.6332  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [11700/20565]  eta: 1:34:23  lr: 0.000020  loss: 2.5912  time: 0.6286  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [11750/20565]  eta: 1:33:51  lr: 0.000020  loss: 3.2270  time: 0.6357  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [11800/20565]  eta: 1:33:20  lr: 0.000020  loss: 2.3258  time: 0.6370  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [11850/20565]  eta: 1:32:47  lr: 0.000020  loss: 3.1143  time: 0.6397  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [11900/20565]  eta: 1:32:15  lr: 0.000020  loss: 3.8536  time: 0.6382  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [11950/20565]  eta: 1:31:43  lr: 0.000020  loss: 2.7986  time: 0.6395  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [12000/20565]  eta: 1:31:11  lr: 0.000020  loss: 2.7256  time: 0.6370  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [12050/20565]  eta: 1:30:39  lr: 0.000020  loss: 3.3144  time: 0.6375  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [12100/20565]  eta: 1:30:07  lr: 0.000020  loss: 2.3654  time: 0.6403  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [12150/20565]  eta: 1:29:35  lr: 0.000020  loss: 2.5992  time: 0.6365  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [12200/20565]  eta: 1:29:03  lr: 0.000020  loss: 2.9994  time: 0.6349  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [12250/20565]  eta: 1:28:31  lr: 0.000020  loss: 2.9477  time: 0.6440  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [12300/20565]  eta: 1:28:00  lr: 0.000020  loss: 2.7135  time: 0.6433  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [12350/20565]  eta: 1:27:28  lr: 0.000020  loss: 2.4068  time: 0.6473  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [12400/20565]  eta: 1:26:55  lr: 0.000020  loss: 2.4857  time: 0.6335  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [12450/20565]  eta: 1:26:23  lr: 0.000020  loss: 2.8215  time: 0.6399  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [12500/20565]  eta: 1:25:51  lr: 0.000020  loss: 2.3438  time: 0.6423  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [12550/20565]  eta: 1:25:20  lr: 0.000020  loss: 3.3445  time: 0.6436  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [12600/20565]  eta: 1:24:48  lr: 0.000020  loss: 3.5871  time: 0.6465  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [12650/20565]  eta: 1:24:16  lr: 0.000020  loss: 3.1311  time: 0.6460  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [12700/20565]  eta: 1:23:44  lr: 0.000020  loss: 2.8560  time: 0.6398  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [12750/20565]  eta: 1:23:12  lr: 0.000020  loss: 2.5721  time: 0.6318  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [12800/20565]  eta: 1:22:40  lr: 0.000020  loss: 2.9549  time: 0.6303  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [12850/20565]  eta: 1:22:08  lr: 0.000020  loss: 2.6702  time: 0.6294  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [12900/20565]  eta: 1:21:36  lr: 0.000020  loss: 2.7073  time: 0.6386  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [12950/20565]  eta: 1:21:04  lr: 0.000020  loss: 2.6498  time: 0.6393  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [13000/20565]  eta: 1:20:32  lr: 0.000020  loss: 2.3680  time: 0.6307  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [13050/20565]  eta: 1:20:00  lr: 0.000020  loss: 3.3294  time: 0.6377  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [13100/20565]  eta: 1:19:28  lr: 0.000020  loss: 2.8516  time: 0.6371  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [13150/20565]  eta: 1:18:56  lr: 0.000020  loss: 2.7206  time: 0.6353  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [13200/20565]  eta: 1:18:24  lr: 0.000020  loss: 2.9961  time: 0.6370  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [13250/20565]  eta: 1:17:52  lr: 0.000020  loss: 2.8515  time: 0.6413  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [13300/20565]  eta: 1:17:20  lr: 0.000020  loss: 2.2053  time: 0.6395  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [13350/20565]  eta: 1:16:48  lr: 0.000020  loss: 2.7890  time: 0.6310  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [13400/20565]  eta: 1:16:16  lr: 0.000020  loss: 2.2016  time: 0.6457  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [13450/20565]  eta: 1:15:44  lr: 0.000020  loss: 3.5581  time: 0.6324  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [13500/20565]  eta: 1:15:12  lr: 0.000020  loss: 2.5119  time: 0.6356  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [13550/20565]  eta: 1:14:40  lr: 0.000020  loss: 3.0582  time: 0.6367  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [13600/20565]  eta: 1:14:08  lr: 0.000020  loss: 2.5976  time: 0.6317  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [13650/20565]  eta: 1:13:36  lr: 0.000020  loss: 3.0069  time: 0.6393  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [13700/20565]  eta: 1:13:04  lr: 0.000020  loss: 3.0991  time: 0.6306  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [13750/20565]  eta: 1:12:32  lr: 0.000020  loss: 3.1180  time: 0.6400  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [13800/20565]  eta: 1:12:00  lr: 0.000020  loss: 3.1152  time: 0.6318  data: 0.0000  max mem: 33640
Train: data epoch: [0]  [13850/20565]  eta: 1:11:28  lr: 0.000020  loss: 3.5246  time: 0.6392  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [13900/20565]  eta: 1:10:56  lr: 0.000020  loss: 3.3335  time: 0.6415  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [13950/20565]  eta: 1:10:34  lr: 0.000020  loss: 2.7524  time: 1.2703  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [14000/20565]  eta: 1:10:17  lr: 0.000020  loss: 2.5318  time: 1.2869  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [14050/20565]  eta: 1:10:00  lr: 0.000020  loss: 2.2334  time: 1.2620  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [14100/20565]  eta: 1:09:42  lr: 0.000020  loss: 3.1988  time: 1.2629  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [14150/20565]  eta: 1:09:23  lr: 0.000020  loss: 2.3131  time: 1.2716  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [14200/20565]  eta: 1:09:05  lr: 0.000020  loss: 1.9944  time: 1.2740  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [14250/20565]  eta: 1:08:46  lr: 0.000020  loss: 2.9561  time: 1.2784  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [14300/20565]  eta: 1:08:27  lr: 0.000020  loss: 2.3898  time: 1.2781  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [14350/20565]  eta: 1:07:59  lr: 0.000020  loss: 2.5541  time: 0.6365  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [14400/20565]  eta: 1:07:26  lr: 0.000020  loss: 2.5165  time: 0.6397  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [14450/20565]  eta: 1:06:53  lr: 0.000020  loss: 2.7740  time: 0.6333  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [14500/20565]  eta: 1:06:20  lr: 0.000020  loss: 2.9951  time: 0.6357  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [14550/20565]  eta: 1:05:46  lr: 0.000020  loss: 2.8898  time: 0.6321  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [14600/20565]  eta: 1:05:13  lr: 0.000020  loss: 2.8144  time: 0.6374  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [14650/20565]  eta: 1:04:40  lr: 0.000020  loss: 2.1306  time: 0.6326  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [14700/20565]  eta: 1:04:07  lr: 0.000020  loss: 2.4578  time: 0.6445  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [14750/20565]  eta: 1:03:34  lr: 0.000020  loss: 3.5897  time: 0.6265  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [14800/20565]  eta: 1:03:00  lr: 0.000020  loss: 2.7521  time: 0.6395  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [14850/20565]  eta: 1:02:27  lr: 0.000020  loss: 3.0834  time: 0.6287  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [14900/20565]  eta: 1:01:54  lr: 0.000020  loss: 2.7688  time: 0.6341  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [14950/20565]  eta: 1:01:21  lr: 0.000020  loss: 2.5185  time: 0.6362  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [15000/20565]  eta: 1:00:48  lr: 0.000020  loss: 2.4716  time: 0.6324  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [15050/20565]  eta: 1:00:15  lr: 0.000020  loss: 3.2982  time: 0.6427  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [15100/20565]  eta: 0:59:42  lr: 0.000020  loss: 4.0533  time: 0.6389  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [15150/20565]  eta: 0:59:08  lr: 0.000020  loss: 3.2517  time: 0.6387  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [15200/20565]  eta: 0:58:35  lr: 0.000020  loss: 2.4319  time: 0.6418  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [15250/20565]  eta: 0:58:03  lr: 0.000020  loss: 3.4275  time: 0.7093  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [15300/20565]  eta: 0:57:30  lr: 0.000020  loss: 2.4893  time: 0.6373  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [15350/20565]  eta: 0:56:57  lr: 0.000020  loss: 2.4458  time: 0.6516  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [15400/20565]  eta: 0:56:24  lr: 0.000020  loss: 2.5599  time: 0.6373  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [15450/20565]  eta: 0:55:51  lr: 0.000020  loss: 2.9901  time: 0.6329  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [15500/20565]  eta: 0:55:17  lr: 0.000020  loss: 2.2765  time: 0.6356  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [15550/20565]  eta: 0:54:44  lr: 0.000020  loss: 2.6084  time: 0.6420  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [15600/20565]  eta: 0:54:15  lr: 0.000020  loss: 2.4645  time: 1.2102  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [15650/20565]  eta: 0:53:52  lr: 0.000020  loss: 2.8893  time: 1.2868  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [15700/20565]  eta: 0:53:29  lr: 0.000020  loss: 2.2407  time: 1.2869  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [15750/20565]  eta: 0:53:06  lr: 0.000020  loss: 2.3840  time: 1.2987  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [15800/20565]  eta: 0:52:42  lr: 0.000020  loss: 3.5890  time: 1.2840  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [15850/20565]  eta: 0:52:18  lr: 0.000020  loss: 2.6384  time: 1.2829  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [15900/20565]  eta: 0:51:53  lr: 0.000020  loss: 2.8795  time: 1.2603  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [15950/20565]  eta: 0:51:29  lr: 0.000020  loss: 2.5032  time: 1.2639  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [16000/20565]  eta: 0:51:04  lr: 0.000020  loss: 2.7799  time: 1.2979  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [16050/20565]  eta: 0:50:39  lr: 0.000020  loss: 2.5751  time: 1.2730  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [16100/20565]  eta: 0:50:14  lr: 0.000020  loss: 2.9145  time: 1.2823  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [16150/20565]  eta: 0:49:48  lr: 0.000020  loss: 2.3734  time: 1.2779  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [16200/20565]  eta: 0:49:22  lr: 0.000020  loss: 1.9172  time: 1.2754  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [16250/20565]  eta: 0:48:56  lr: 0.000020  loss: 3.4019  time: 1.2869  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [16300/20565]  eta: 0:48:30  lr: 0.000020  loss: 2.5533  time: 1.2675  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [16350/20565]  eta: 0:48:04  lr: 0.000020  loss: 3.1280  time: 1.2791  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [16400/20565]  eta: 0:47:37  lr: 0.000020  loss: 2.4282  time: 1.2758  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [16450/20565]  eta: 0:47:10  lr: 0.000020  loss: 2.5470  time: 1.2688  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [16500/20565]  eta: 0:46:43  lr: 0.000020  loss: 2.8773  time: 1.2849  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [16550/20565]  eta: 0:46:16  lr: 0.000020  loss: 2.8253  time: 1.2933  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [16600/20565]  eta: 0:45:48  lr: 0.000020  loss: 2.9182  time: 1.2611  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [16650/20565]  eta: 0:45:20  lr: 0.000020  loss: 2.9672  time: 1.2691  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [16700/20565]  eta: 0:44:52  lr: 0.000020  loss: 2.6978  time: 1.2999  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [16750/20565]  eta: 0:44:24  lr: 0.000020  loss: 2.7206  time: 1.2833  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [16800/20565]  eta: 0:43:56  lr: 0.000020  loss: 2.0497  time: 1.2670  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [16850/20565]  eta: 0:43:27  lr: 0.000020  loss: 2.2850  time: 1.2864  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [16900/20565]  eta: 0:42:58  lr: 0.000020  loss: 2.2888  time: 1.2621  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [16950/20565]  eta: 0:42:29  lr: 0.000020  loss: 3.0857  time: 1.2731  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [17000/20565]  eta: 0:42:00  lr: 0.000020  loss: 2.6280  time: 1.2901  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [17050/20565]  eta: 0:41:30  lr: 0.000020  loss: 2.9121  time: 1.2855  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [17100/20565]  eta: 0:41:00  lr: 0.000020  loss: 2.6190  time: 1.2898  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [17150/20565]  eta: 0:40:30  lr: 0.000020  loss: 2.7156  time: 1.2489  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [17200/20565]  eta: 0:40:00  lr: 0.000020  loss: 1.8213  time: 1.2914  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [17250/20565]  eta: 0:39:29  lr: 0.000020  loss: 2.8705  time: 1.2765  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [17300/20565]  eta: 0:38:59  lr: 0.000020  loss: 4.0351  time: 1.2962  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [17350/20565]  eta: 0:38:28  lr: 0.000020  loss: 2.4656  time: 1.2785  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [17400/20565]  eta: 0:37:57  lr: 0.000020  loss: 2.6110  time: 1.2779  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [17450/20565]  eta: 0:37:26  lr: 0.000020  loss: 3.5271  time: 1.2887  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [17500/20565]  eta: 0:36:55  lr: 0.000020  loss: 3.1937  time: 1.2654  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [17550/20565]  eta: 0:36:24  lr: 0.000020  loss: 3.4327  time: 1.2850  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [17600/20565]  eta: 0:35:52  lr: 0.000020  loss: 2.1120  time: 1.2493  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [17650/20565]  eta: 0:35:20  lr: 0.000020  loss: 2.8279  time: 1.2759  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [17700/20565]  eta: 0:34:48  lr: 0.000020  loss: 2.7445  time: 1.2888  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [17750/20565]  eta: 0:34:16  lr: 0.000020  loss: 2.8398  time: 1.2723  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [17800/20565]  eta: 0:33:44  lr: 0.000020  loss: 2.8115  time: 1.2813  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [17850/20565]  eta: 0:33:12  lr: 0.000020  loss: 2.8375  time: 1.2864  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [17900/20565]  eta: 0:32:39  lr: 0.000020  loss: 2.4500  time: 1.2990  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [17950/20565]  eta: 0:32:06  lr: 0.000020  loss: 3.1591  time: 1.2773  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [18000/20565]  eta: 0:31:33  lr: 0.000020  loss: 3.1890  time: 1.2778  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [18050/20565]  eta: 0:31:00  lr: 0.000020  loss: 3.2843  time: 1.2519  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [18100/20565]  eta: 0:30:27  lr: 0.000020  loss: 2.1344  time: 1.2562  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [18150/20565]  eta: 0:29:53  lr: 0.000020  loss: 2.6797  time: 1.2438  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [18200/20565]  eta: 0:29:19  lr: 0.000020  loss: 2.6032  time: 1.2671  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [18250/20565]  eta: 0:28:46  lr: 0.000020  loss: 2.6853  time: 1.2861  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [18300/20565]  eta: 0:28:11  lr: 0.000020  loss: 3.1389  time: 1.2427  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [18350/20565]  eta: 0:27:37  lr: 0.000020  loss: 2.9459  time: 1.2043  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [18400/20565]  eta: 0:27:03  lr: 0.000020  loss: 2.3720  time: 1.2557  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [18450/20565]  eta: 0:26:28  lr: 0.000020  loss: 1.6233  time: 1.2850  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [18500/20565]  eta: 0:25:53  lr: 0.000020  loss: 2.2697  time: 1.2672  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [18550/20565]  eta: 0:25:19  lr: 0.000020  loss: 3.0083  time: 1.2801  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [18600/20565]  eta: 0:24:44  lr: 0.000020  loss: 2.7253  time: 1.2733  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [18650/20565]  eta: 0:24:09  lr: 0.000020  loss: 2.9167  time: 1.2843  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [18700/20565]  eta: 0:23:33  lr: 0.000020  loss: 2.6226  time: 1.2880  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [18750/20565]  eta: 0:22:58  lr: 0.000020  loss: 3.1791  time: 1.2934  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [18800/20565]  eta: 0:22:23  lr: 0.000020  loss: 2.8706  time: 1.2861  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [18850/20565]  eta: 0:21:47  lr: 0.000020  loss: 2.9825  time: 1.2844  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [18900/20565]  eta: 0:21:11  lr: 0.000020  loss: 3.3667  time: 1.2765  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [18950/20565]  eta: 0:20:35  lr: 0.000020  loss: 2.5406  time: 1.2686  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [19000/20565]  eta: 0:19:59  lr: 0.000020  loss: 1.7039  time: 1.2852  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [19050/20565]  eta: 0:19:23  lr: 0.000020  loss: 3.1392  time: 1.2848  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [19100/20565]  eta: 0:18:46  lr: 0.000020  loss: 3.0176  time: 1.3877  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [19150/20565]  eta: 0:18:10  lr: 0.000020  loss: 2.9987  time: 1.2728  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [19200/20565]  eta: 0:17:33  lr: 0.000020  loss: 3.3541  time: 1.2910  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [19250/20565]  eta: 0:16:56  lr: 0.000020  loss: 2.6651  time: 1.2664  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [19300/20565]  eta: 0:16:19  lr: 0.000020  loss: 2.2888  time: 1.2539  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [19350/20565]  eta: 0:15:42  lr: 0.000020  loss: 2.2893  time: 1.2858  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [19400/20565]  eta: 0:15:05  lr: 0.000020  loss: 3.0608  time: 1.2043  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [19450/20565]  eta: 0:14:27  lr: 0.000020  loss: 3.2615  time: 1.2173  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [19500/20565]  eta: 0:13:49  lr: 0.000020  loss: 2.4502  time: 1.1998  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [19550/20565]  eta: 0:13:12  lr: 0.000020  loss: 3.2278  time: 1.2347  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [19600/20565]  eta: 0:12:34  lr: 0.000020  loss: 2.7232  time: 1.2521  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [19650/20565]  eta: 0:11:56  lr: 0.000020  loss: 2.4604  time: 1.1914  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [19700/20565]  eta: 0:11:17  lr: 0.000020  loss: 2.8249  time: 1.2327  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [19750/20565]  eta: 0:10:39  lr: 0.000020  loss: 2.2736  time: 1.2473  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [19800/20565]  eta: 0:10:01  lr: 0.000020  loss: 2.4870  time: 1.2571  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [19850/20565]  eta: 0:09:22  lr: 0.000020  loss: 2.9708  time: 1.2448  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [19900/20565]  eta: 0:08:44  lr: 0.000020  loss: 2.2377  time: 1.2170  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [19950/20565]  eta: 0:08:05  lr: 0.000020  loss: 3.0309  time: 1.2524  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [20000/20565]  eta: 0:07:26  lr: 0.000020  loss: 3.4228  time: 1.2548  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [20050/20565]  eta: 0:06:47  lr: 0.000020  loss: 2.6171  time: 1.2528  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [20100/20565]  eta: 0:06:08  lr: 0.000020  loss: 1.9888  time: 1.1828  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [20150/20565]  eta: 0:05:29  lr: 0.000020  loss: 2.8523  time: 1.2060  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [20200/20565]  eta: 0:04:50  lr: 0.000020  loss: 3.0041  time: 1.2307  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [20250/20565]  eta: 0:04:10  lr: 0.000020  loss: 3.0731  time: 1.2078  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [20300/20565]  eta: 0:03:31  lr: 0.000020  loss: 2.7309  time: 1.1420  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [20350/20565]  eta: 0:02:51  lr: 0.000020  loss: 2.3489  time: 1.2005  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [20400/20565]  eta: 0:02:11  lr: 0.000020  loss: 2.8969  time: 1.2186  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [20450/20565]  eta: 0:01:31  lr: 0.000020  loss: 2.7931  time: 1.1827  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [20500/20565]  eta: 0:00:52  lr: 0.000020  loss: 2.6309  time: 1.1998  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [20550/20565]  eta: 0:00:12  lr: 0.000020  loss: 2.5760  time: 1.2253  data: 0.0000  max mem: 34285
Train: data epoch: [0]  [20564/20565]  eta: 0:00:00  lr: 0.000020  loss: 3.0666  time: 1.2225  data: 0.0000  max mem: 34285
Train: data epoch: [0] Total time: 4:34:54 (0.8020 s / it)
2023-09-18 12:41:06,773 [INFO] Averaged stats: lr: 0.0000  loss: 2.8522
2023-09-18 12:41:06,776 [INFO] No validation splits found.
2023-09-18 12:41:06,801 [INFO] Saving checkpoint at epoch 0 to /home/tsingqguo/wwx/paper_worksapce/LAVIS/lavis/output/BLIP/VQA/mnt/local/wwx/Output/LAVIS/VQA_FT/20230918080/checkpoint_0.pth.
Traceback (most recent call last):
  File "train.py", line 103, in <module>
    main()
  File "train.py", line 99, in main
    runner.train()
  File "/home/tsingqguo/wwx/paper_worksapce/LAVIS/lavis/runners/runner_base.py", line 403, in train
    dist.barrier()
  File "/home/tsingqguo/anaconda3/envs/lavis_v2/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 3327, in barrier
    default_pg = _get_default_group()
  File "/home/tsingqguo/anaconda3/envs/lavis_v2/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 707, in _get_default_group
    raise RuntimeError(
RuntimeError: Default process group has not been initialized, please make sure to call init_process_group.
